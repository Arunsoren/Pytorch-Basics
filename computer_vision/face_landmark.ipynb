{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLIB Dataset\n",
    "-6666 images of varying dimensions\n",
    "-68 landmarks for each face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the DLIB Datset\n",
    "%%capture\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\n",
    "    !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz'    \n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Data\n",
    "file = open('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.pts')\n",
    "points = file.readlines()[3:-1]\n",
    "\n",
    "lanfmarks = []\n",
    "\n",
    "for points in points:\n",
    "    x,y = point.split(' ')\n",
    "    landmarks.append([floor(float(x)), floor(float(y[:-1]))])\n",
    "\n",
    "landmarks = np.array(landmarks)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(mpimg.imread('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.jpg'))\n",
    "plt.scatter(landmarks[: ,0], landmarks[:, 1], s=5, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cropping the Image to get only faces with same dimentions\n",
    "class Transform():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))],\n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 0.5     #center of image\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 0.5\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandMarkDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "        for filename in root[2]:\n",
    "            self.image_filenmaes.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            landmarks = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmarks.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')\n",
    "        assert len(self.image_filenmaes) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenmaes[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "\n",
    "        if self.tranform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "        landmarks = landmarks -0.5\n",
    "        return image, landmarks\n",
    "\n",
    "dataset = FaceLandMarkDataset(Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transforms\n",
    "image, landmarks = dataset[0]\n",
    "landmarks = (landmarks +0.5) *224 \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image.numpy().squeeze(), cmap='gray')\n",
    "plt.scatter(landmarks[:, 0], landmarks[:,1], s=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data \n",
    "len_valid_set = int(0.1*len(dataset))\n",
    "len_train_set = len(dataset) - len_valid_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The legth of Valid set is {}\".format(len_valid_set))\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [len_train_set, len_valid_set])\n",
    "\n",
    "# Shuffle\n",
    "train_loader = torch.utils.data.Dataloader(train_dataset, batch_size=64, shuffle= True, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle= True, num_workers =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmarks = next(iter(train_loader))\n",
    "\n",
    "print(image.shape)\n",
    "print(landmarks.shape)\n",
    "\n",
    "torch.size([64, 1, 224, 224])\n",
    "torch.size([34, 68, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes=136):\n",
    "        super().__init__()\n",
    "        self.model_name = 'resnet18'\n",
    "        self.model = models.resnet18()\n",
    "        self.model.conv1 = nn.Conv2d(1, 664, kernel_size=7, stride=2, padding=3, bias =False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        #input channel as 1 to accept grey scale images\n",
    "        #final layer output 68*2=136 (68 landmarks)\n",
    "\n",
    "     def forward(self, x):\n",
    "         x = self.model(x)\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "\n",
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d Loss: %.4f\" %(step, total_step, loss))\n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d Loss: %.4f\" %(step, total_step, loss))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ARUNKU~1\\AppData\\Local\\Temp/ipykernel_19612/2481542078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "network = Network()\n",
    "network.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr =0.0001)\n",
    "\n",
    "loss_min = np.inf\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    network.train()\n",
    "    for steps in range(1, len(train_loader)+1):\n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        images = images.cuda()\n",
    "        landmarks = landmarks.view(landmarks.size(0), -1).cuda()\n",
    "        predictions = network(images)\n",
    "        # Clearing the Gradient before calculating \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # find the loss for current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "\n",
    "        #calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "\n",
    "        #Updating the Pramas\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for step in range(1, len(valid_loader)+1):\n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "            images = images.cuda()\n",
    "            landmarks = landmarks.view(landmarks.size(0), -1).cuda()\n",
    "            predictions = networks(images)\n",
    "\n",
    "            #find the loss curr step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, \"valid\")\n",
    "        \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "\n",
    "    print('\\n --------------------')\n",
    "    print('Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('-----------------------')\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.stet_dict(), '/content/face_landmarks.pth')\n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "\n",
    "print(\"Training Complete\")\n",
    "print(\"Total Elapsed Time: {}s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test Images\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    best_network = Network()\n",
    "    best_network.cuda()\n",
    "    best_network.load_state_dict(torch.load('/content/face_landmarks.pth'))\n",
    "    best_network.eval()\n",
    "\n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    images = images.cuda()\n",
    "\n",
    "    landmarks = (landmarks+0.5) *224\n",
    "    predictions = (best_network(images).cpu()+0.5)*224\n",
    "    predictions = predictions.view(-1,68,2)\n",
    "\n",
    "    plt.figure(figsize=(10,40))\n",
    "\n",
    "    for img_num in range(8):\n",
    "        plt.subplot(8,1, img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap = 'gray')\n",
    "        plt.scatter(predictions[img_num,:,0], predictions[img_num,:, 1], c ='r', s=5)\n",
    "        plt.scatter(landmarks[img_num, :, 0], landmarks[img_num, :,1], c= 'g', s =5)\n",
    "\n",
    "print('Total number of test images:{}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time:{}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f70e6d688a45095d61457b4d09636a28b3c47e26b0acf96e1a315d8d5248cf7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
